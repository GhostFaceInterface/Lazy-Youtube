{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/120], Train Loss: 5.9117, Test Loss: 5.6907, Train Acc: 22.97%, Test Acc: 72.41%\n",
      "Epoch [2/120], Train Loss: 5.4536, Test Loss: 5.1503, Train Acc: 83.72%, Test Acc: 81.61%\n",
      "Epoch [3/120], Train Loss: 4.7835, Test Loss: 4.3476, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [4/120], Train Loss: 3.7909, Test Loss: 3.2369, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [5/120], Train Loss: 2.5957, Test Loss: 2.0676, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [6/120], Train Loss: 1.6094, Test Loss: 1.2821, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [7/120], Train Loss: 1.0191, Test Loss: 0.8777, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [8/120], Train Loss: 0.7269, Test Loss: 0.6895, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [9/120], Train Loss: 0.5866, Test Loss: 0.5978, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [10/120], Train Loss: 0.5235, Test Loss: 0.5430, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [11/120], Train Loss: 0.4896, Test Loss: 0.5154, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [12/120], Train Loss: 0.4615, Test Loss: 0.4971, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [13/120], Train Loss: 0.4428, Test Loss: 0.4806, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [14/120], Train Loss: 0.4358, Test Loss: 0.4708, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [15/120], Train Loss: 0.4277, Test Loss: 0.4650, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [16/120], Train Loss: 0.4099, Test Loss: 0.4536, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [17/120], Train Loss: 0.4051, Test Loss: 0.4476, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [18/120], Train Loss: 0.3982, Test Loss: 0.4470, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [19/120], Train Loss: 0.3982, Test Loss: 0.4406, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [20/120], Train Loss: 0.3861, Test Loss: 0.4335, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [21/120], Train Loss: 0.3836, Test Loss: 0.4309, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [22/120], Train Loss: 0.3871, Test Loss: 0.4294, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [23/120], Train Loss: 0.3750, Test Loss: 0.4238, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [24/120], Train Loss: 0.3692, Test Loss: 0.4207, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [25/120], Train Loss: 0.3688, Test Loss: 0.4182, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [26/120], Train Loss: 0.3620, Test Loss: 0.4187, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [27/120], Train Loss: 0.3599, Test Loss: 0.4143, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [28/120], Train Loss: 0.3633, Test Loss: 0.4157, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [29/120], Train Loss: 0.3612, Test Loss: 0.4068, Train Acc: 84.01%, Test Acc: 81.61%\n",
      "Epoch [30/120], Train Loss: 0.3524, Test Loss: 0.4065, Train Acc: 84.30%, Test Acc: 81.61%\n",
      "Epoch [31/120], Train Loss: 0.3500, Test Loss: 0.4031, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [32/120], Train Loss: 0.3456, Test Loss: 0.4006, Train Acc: 84.30%, Test Acc: 81.61%\n",
      "Epoch [33/120], Train Loss: 0.3441, Test Loss: 0.3990, Train Acc: 84.59%, Test Acc: 82.76%\n",
      "Epoch [34/120], Train Loss: 0.3433, Test Loss: 0.3980, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [35/120], Train Loss: 0.3387, Test Loss: 0.3938, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [36/120], Train Loss: 0.3393, Test Loss: 0.3907, Train Acc: 84.59%, Test Acc: 82.76%\n",
      "Epoch [37/120], Train Loss: 0.3345, Test Loss: 0.3870, Train Acc: 84.30%, Test Acc: 80.46%\n",
      "Epoch [38/120], Train Loss: 0.3302, Test Loss: 0.3865, Train Acc: 84.01%, Test Acc: 81.61%\n",
      "Epoch [39/120], Train Loss: 0.3271, Test Loss: 0.3820, Train Acc: 84.59%, Test Acc: 81.61%\n",
      "Epoch [40/120], Train Loss: 0.3249, Test Loss: 0.3806, Train Acc: 84.59%, Test Acc: 82.76%\n",
      "Epoch [41/120], Train Loss: 0.3308, Test Loss: 0.3762, Train Acc: 84.88%, Test Acc: 81.61%\n",
      "Epoch [42/120], Train Loss: 0.3216, Test Loss: 0.3739, Train Acc: 85.17%, Test Acc: 83.91%\n",
      "Epoch [43/120], Train Loss: 0.3175, Test Loss: 0.3711, Train Acc: 85.17%, Test Acc: 82.76%\n",
      "Epoch [44/120], Train Loss: 0.3117, Test Loss: 0.3666, Train Acc: 84.88%, Test Acc: 82.76%\n",
      "Epoch [45/120], Train Loss: 0.3091, Test Loss: 0.3643, Train Acc: 85.76%, Test Acc: 83.91%\n",
      "Epoch [46/120], Train Loss: 0.3100, Test Loss: 0.3600, Train Acc: 86.05%, Test Acc: 83.91%\n",
      "Epoch [47/120], Train Loss: 0.3002, Test Loss: 0.3581, Train Acc: 85.17%, Test Acc: 81.61%\n",
      "Epoch [48/120], Train Loss: 0.3054, Test Loss: 0.3548, Train Acc: 85.47%, Test Acc: 83.91%\n",
      "Epoch [49/120], Train Loss: 0.2970, Test Loss: 0.3499, Train Acc: 85.47%, Test Acc: 80.46%\n",
      "Epoch [50/120], Train Loss: 0.2956, Test Loss: 0.3462, Train Acc: 86.34%, Test Acc: 82.76%\n",
      "Epoch [51/120], Train Loss: 0.2936, Test Loss: 0.3424, Train Acc: 86.05%, Test Acc: 82.76%\n",
      "Epoch [52/120], Train Loss: 0.2868, Test Loss: 0.3368, Train Acc: 85.76%, Test Acc: 82.76%\n",
      "Epoch [53/120], Train Loss: 0.2833, Test Loss: 0.3321, Train Acc: 86.05%, Test Acc: 83.91%\n",
      "Epoch [54/120], Train Loss: 0.2866, Test Loss: 0.3291, Train Acc: 86.05%, Test Acc: 82.76%\n",
      "Epoch [55/120], Train Loss: 0.2756, Test Loss: 0.3232, Train Acc: 86.34%, Test Acc: 83.91%\n",
      "Epoch [56/120], Train Loss: 0.2707, Test Loss: 0.3187, Train Acc: 87.21%, Test Acc: 83.91%\n",
      "Epoch [57/120], Train Loss: 0.2664, Test Loss: 0.3145, Train Acc: 86.63%, Test Acc: 83.91%\n",
      "Epoch [58/120], Train Loss: 0.2612, Test Loss: 0.3102, Train Acc: 86.92%, Test Acc: 82.76%\n",
      "Epoch [59/120], Train Loss: 0.2583, Test Loss: 0.3030, Train Acc: 86.63%, Test Acc: 83.91%\n",
      "Epoch [60/120], Train Loss: 0.2598, Test Loss: 0.2993, Train Acc: 86.34%, Test Acc: 83.91%\n",
      "Epoch [61/120], Train Loss: 0.2535, Test Loss: 0.2938, Train Acc: 86.92%, Test Acc: 83.91%\n",
      "Epoch [62/120], Train Loss: 0.2484, Test Loss: 0.2905, Train Acc: 87.50%, Test Acc: 83.91%\n",
      "Epoch [63/120], Train Loss: 0.2434, Test Loss: 0.2856, Train Acc: 87.50%, Test Acc: 83.91%\n",
      "Epoch [64/120], Train Loss: 0.2402, Test Loss: 0.2832, Train Acc: 86.92%, Test Acc: 82.76%\n",
      "Epoch [65/120], Train Loss: 0.2354, Test Loss: 0.2774, Train Acc: 87.21%, Test Acc: 83.91%\n",
      "Epoch [66/120], Train Loss: 0.2298, Test Loss: 0.2702, Train Acc: 87.50%, Test Acc: 85.06%\n",
      "Epoch [67/120], Train Loss: 0.2310, Test Loss: 0.2676, Train Acc: 86.92%, Test Acc: 85.06%\n",
      "Epoch [68/120], Train Loss: 0.2253, Test Loss: 0.2636, Train Acc: 87.21%, Test Acc: 86.21%\n",
      "Epoch [69/120], Train Loss: 0.2203, Test Loss: 0.2585, Train Acc: 88.08%, Test Acc: 86.21%\n",
      "Epoch [70/120], Train Loss: 0.2176, Test Loss: 0.2537, Train Acc: 88.66%, Test Acc: 88.51%\n",
      "Epoch [71/120], Train Loss: 0.2103, Test Loss: 0.2504, Train Acc: 88.95%, Test Acc: 88.51%\n",
      "Epoch [72/120], Train Loss: 0.2094, Test Loss: 0.2463, Train Acc: 89.24%, Test Acc: 89.66%\n",
      "Epoch [73/120], Train Loss: 0.2064, Test Loss: 0.2446, Train Acc: 89.83%, Test Acc: 87.36%\n",
      "Epoch [74/120], Train Loss: 0.2020, Test Loss: 0.2391, Train Acc: 90.70%, Test Acc: 90.80%\n",
      "Epoch [75/120], Train Loss: 0.2052, Test Loss: 0.2378, Train Acc: 91.28%, Test Acc: 90.80%\n",
      "Epoch [76/120], Train Loss: 0.2016, Test Loss: 0.2324, Train Acc: 91.57%, Test Acc: 90.80%\n",
      "Epoch [77/120], Train Loss: 0.1939, Test Loss: 0.2288, Train Acc: 91.28%, Test Acc: 90.80%\n",
      "Epoch [78/120], Train Loss: 0.1900, Test Loss: 0.2271, Train Acc: 93.02%, Test Acc: 89.66%\n",
      "Epoch [79/120], Train Loss: 0.1904, Test Loss: 0.2258, Train Acc: 93.90%, Test Acc: 89.66%\n",
      "Epoch [80/120], Train Loss: 0.1883, Test Loss: 0.2230, Train Acc: 92.15%, Test Acc: 90.80%\n",
      "Epoch [81/120], Train Loss: 0.1875, Test Loss: 0.2219, Train Acc: 92.15%, Test Acc: 90.80%\n",
      "Epoch [82/120], Train Loss: 0.1822, Test Loss: 0.2178, Train Acc: 92.73%, Test Acc: 90.80%\n",
      "Epoch [83/120], Train Loss: 0.1794, Test Loss: 0.2157, Train Acc: 92.44%, Test Acc: 89.66%\n",
      "Epoch [84/120], Train Loss: 0.1789, Test Loss: 0.2114, Train Acc: 94.19%, Test Acc: 89.66%\n",
      "Epoch [85/120], Train Loss: 0.1784, Test Loss: 0.2109, Train Acc: 93.60%, Test Acc: 90.80%\n",
      "Epoch [86/120], Train Loss: 0.1716, Test Loss: 0.2116, Train Acc: 93.60%, Test Acc: 90.80%\n",
      "Epoch [87/120], Train Loss: 0.1725, Test Loss: 0.2117, Train Acc: 93.02%, Test Acc: 90.80%\n",
      "Epoch [88/120], Train Loss: 0.1684, Test Loss: 0.2099, Train Acc: 93.60%, Test Acc: 90.80%\n",
      "Epoch [89/120], Train Loss: 0.1738, Test Loss: 0.2055, Train Acc: 94.77%, Test Acc: 90.80%\n",
      "Epoch [90/120], Train Loss: 0.1696, Test Loss: 0.2023, Train Acc: 92.73%, Test Acc: 90.80%\n",
      "Epoch [91/120], Train Loss: 0.1640, Test Loss: 0.2027, Train Acc: 93.60%, Test Acc: 90.80%\n",
      "Epoch [92/120], Train Loss: 0.1608, Test Loss: 0.2012, Train Acc: 94.19%, Test Acc: 90.80%\n",
      "Epoch [93/120], Train Loss: 0.1603, Test Loss: 0.2001, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [94/120], Train Loss: 0.1560, Test Loss: 0.1979, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [95/120], Train Loss: 0.1578, Test Loss: 0.1963, Train Acc: 94.19%, Test Acc: 90.80%\n",
      "Epoch [96/120], Train Loss: 0.1597, Test Loss: 0.1955, Train Acc: 94.77%, Test Acc: 90.80%\n",
      "Epoch [97/120], Train Loss: 0.1619, Test Loss: 0.2011, Train Acc: 94.19%, Test Acc: 90.80%\n",
      "Epoch [98/120], Train Loss: 0.1564, Test Loss: 0.1962, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [99/120], Train Loss: 0.1562, Test Loss: 0.1943, Train Acc: 93.90%, Test Acc: 90.80%\n",
      "Epoch [100/120], Train Loss: 0.1499, Test Loss: 0.1955, Train Acc: 95.64%, Test Acc: 89.66%\n",
      "Epoch [101/120], Train Loss: 0.1500, Test Loss: 0.1894, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [102/120], Train Loss: 0.1473, Test Loss: 0.1926, Train Acc: 93.90%, Test Acc: 90.80%\n",
      "Epoch [103/120], Train Loss: 0.1479, Test Loss: 0.1930, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [104/120], Train Loss: 0.1497, Test Loss: 0.1920, Train Acc: 94.48%, Test Acc: 91.95%\n",
      "Epoch [105/120], Train Loss: 0.1444, Test Loss: 0.1867, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [106/120], Train Loss: 0.1446, Test Loss: 0.1895, Train Acc: 93.90%, Test Acc: 90.80%\n",
      "Epoch [107/120], Train Loss: 0.1430, Test Loss: 0.1915, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [108/120], Train Loss: 0.1426, Test Loss: 0.1890, Train Acc: 94.77%, Test Acc: 90.80%\n",
      "Epoch [109/120], Train Loss: 0.1440, Test Loss: 0.1863, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [110/120], Train Loss: 0.1399, Test Loss: 0.1874, Train Acc: 94.48%, Test Acc: 91.95%\n",
      "Epoch [111/120], Train Loss: 0.1383, Test Loss: 0.1879, Train Acc: 94.77%, Test Acc: 90.80%\n",
      "Epoch [112/120], Train Loss: 0.1421, Test Loss: 0.1863, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [113/120], Train Loss: 0.1369, Test Loss: 0.1864, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [114/120], Train Loss: 0.1377, Test Loss: 0.1839, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [115/120], Train Loss: 0.1389, Test Loss: 0.1887, Train Acc: 94.77%, Test Acc: 91.95%\n",
      "Epoch [116/120], Train Loss: 0.1331, Test Loss: 0.1822, Train Acc: 94.77%, Test Acc: 90.80%\n",
      "Epoch [117/120], Train Loss: 0.1347, Test Loss: 0.1844, Train Acc: 94.77%, Test Acc: 90.80%\n",
      "Epoch [118/120], Train Loss: 0.1368, Test Loss: 0.1857, Train Acc: 94.48%, Test Acc: 90.80%\n",
      "Epoch [119/120], Train Loss: 0.1315, Test Loss: 0.1858, Train Acc: 94.48%, Test Acc: 91.95%\n",
      "Epoch [120/120], Train Loss: 0.1310, Test Loss: 0.1831, Train Acc: 95.06%, Test Acc: 90.80%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset\n",
    "def create_dataloaders(data_path, batch_size=32, train_split=0.8):\n",
    "    data = pd.read_csv(data_path)\n",
    "    features = data.iloc[:, :-1].values\n",
    "    labels = data.iloc[:, -1].values\n",
    "\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, features, labels):\n",
    "            self.features = torch.tensor(features, dtype=torch.float32)\n",
    "            self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.features)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.features[idx], self.labels[idx]\n",
    "\n",
    "    # Shuffle and split data\n",
    "    indices = np.arange(len(features))\n",
    "    np.random.shuffle(indices)\n",
    "    split_idx = int(len(indices) * train_split)\n",
    "\n",
    "    train_indices = indices[:split_idx]\n",
    "    test_indices = indices[split_idx:]\n",
    "\n",
    "    train_dataset = CustomDataset(features[train_indices], labels[train_indices])\n",
    "    test_dataset = CustomDataset(features[test_indices], labels[test_indices])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader, features.shape[1], len(np.unique(labels))\n",
    "\n",
    "# LSTM Model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add a sequence dimension\n",
    "        h_0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
    "        c_0 = torch.zeros(1, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        _, (hidden, _) = self.lstm(x, (h_0, c_0))\n",
    "        out = self.fc(hidden[-1])\n",
    "        return out\n",
    "\n",
    "# Training and Evaluation Functions\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(100 * correct_train / total_train)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for features, labels in test_loader:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        test_accuracies.append(100 * correct_test / total_test)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.2f}%, Test Acc: {test_accuracies[-1]:.2f}%\")\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# Plot Loss and Accuracy Graphs\n",
    "def plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(test_losses, label=\"Test Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution\n",
    "data_path = \"data/gestures.csv\"  # Change to your dataset path\n",
    "batch_size = 32\n",
    "hidden_size = 128\n",
    "epochs = 120\n",
    "\n",
    "train_loader, test_loader, input_size, num_classes = create_dataloaders(data_path, batch_size)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train_model(model, train_loader, test_loader, criterion, optimizer, epochs)\n",
    "plot_metrics(train_losses, test_losses, train_accuracies, test_accuracies)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"lstm_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
